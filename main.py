import numpy as np
import logging
import json
import time
import os
import asyncio
import requests
import pandas as pd
import io
import csv
import codecs
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone, time as dt_time
from fastapi.middleware.cors import CORSMiddleware
from fastapi import Response, Request
# Import Mobius client and configuration
from mobius_client import create_content_instance, retrieve_all_content_instances, retrieve_latest_content_instance
from config import AE_NAME, MOCK_DATA_MODE, REQUEST_TIMEOUT, CNT_STATUS, CNT_NOISE, CNT_RAW, CNT_APOLOGY
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
# Import AI model functions
from ai_engine import load_ai_model_v2, predict_noise_v2, preprocess_audio_for_v2

all_analysis_history = []
MOBIUS_URL = "https://onem2m.iotcoss.ac.kr/Mobius/ae_Namsan/cnt_noise/la"
HEADERS = {
    "Accept": "application/json",
    "X-M2M-RI": "12345",
    "X-M2M-Origin": "AWAYXoieop5ncAjTh90YfkHk9eH8Z7Vb",
    "X-API-KEY": "AWAYXoieop5ncAjTh90YfkHk9eH8Z7Vb",
    "x-auth-custom-lecture": "LCT_20250007",
    "x-auth-custom-creator": "dgunamsan"
}

# --- Configure Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

from collections import deque

# --- State Management & Constants ---
class HouseState(BaseModel):
    status: str = "quiet"  # quiet, loud
    start_time: datetime | None = None
    last_loud_time: datetime | None = None
    last_packet_severity: str = "Green"
    start_of_quiet: datetime | None = None
    mediation_active: bool = False
    mediation_sent_time: str | None = None
    apology_active: bool = False
    apology_sent_time: str | None = None
    lmax_exceed_count: int = 0 # Lmax ì´ˆê³¼ íšŸìˆ˜ ì¹´ìš´íŠ¸

house_states: Dict[str, HouseState] = {}
QUIET_PERIOD_SECONDS = 5
VIBRATION_PEAK_THRESHOLD = 0.3

# --- Moving Average Buffers ---
noise_buffer: Dict[str, deque] = {} # {house_id: deque([db, db, ...], maxlen=300)}

def update_noise_metrics(house_id: str, current_db: float):
    if house_id not in noise_buffer:
        noise_buffer[house_id] = deque(maxlen=300) # ìµœëŒ€ 5ë¶„(1ì´ˆë‹¹ 1ê°œ ê¸°ì¤€ 300ê°œ)
    
    noise_buffer[house_id].append(current_db)
    data = list(noise_buffer[house_id])
    
    # 1ë¶„ í‰ê·  (ìµœê·¼ 60ê°œ)
    avg_1min = sum(data[-60:]) / len(data[-60:]) if len(data) >= 60 else sum(data)/len(data)
    # 5ë¶„ í‰ê·  (ì „ì²´ 300ê°œ)
    avg_5min = sum(data) / len(data)
    
    return round(avg_1min, 2), round(avg_5min, 2)

# --- Pydantic Models ---
class MetaData(BaseModel):
    sampling_rate: str
    vibration_unit: str
    sound_unit: str

class SensorPayload(BaseModel):
    vibration: Dict[str, List[float]]
    sound_raw: List[int]
    raw_max_amplitude: int

class NewPayload(BaseModel):
    model_config = ConfigDict(extra='ignore')
    house_id: str
    timestamp: str
    meta: Any
    payload: Any 

class AnalysisResult(BaseModel):
    result: str
    probability: float
    db_level: float
    avg_1min: float = 0.0
    avg_5min: float = 0.0
    severity: str
    is_external: bool = False
    duration: float = 0.0
    vibration_peaks: int = 0
    vibration_max: float = 0.0
    audio_signature: List[float] = []

class Action(BaseModel):
    mediation_sent: bool
    target: str

class ApologyDetail(BaseModel):
    sent: bool
    timestamp: str

class OneM2MPlatformOutput(BaseModel):
    event_id: str
    house_id: str
    timestamp: str
    analysis: AnalysisResult
    action: Action
    apology_detail: Optional[ApologyDetail] = None

class MobiusNotification(BaseModel):
    m2m_sgn: Dict[str, Any] = Field(None, alias="m2m:sgn")
    sgn: Dict[str, Any] = None

# --- Mock Data Generation ---
async def generate_mock_output_data() -> OneM2MPlatformOutput:
    house_id = "dgu_house_3140"
    current_time = datetime.now()
    results = ["Footstep", "Impact Noise", "Voice", "Silence"]
    severities = ["Green", "Yellow", "Red"]
    result = np.random.choice(results)
    severity = np.random.choice(severities, p=[0.6, 0.3, 0.1])
    db_level = round(np.random.uniform(40, 90), 2)
    if severity == "Green": db_level = round(np.random.uniform(30, 50), 2)
    elif severity == "Red": db_level = round(np.random.uniform(80, 100), 2)
    probability = round(np.random.uniform(0.3, 0.99), 2)
    mediation_sent = severity in ["Yellow", "Red"]
    return OneM2MPlatformOutput(
        event_id=f"MOCK_EVT_{current_time.strftime('%Y%m%d_%H%M%S_%f')}",
        house_id=house_id, timestamp=current_time.isoformat(),
        analysis=AnalysisResult(result=result, probability=probability, db_level=db_level, severity=severity),
        action=Action(mediation_sent=mediation_sent, target=severity)
    )

async def mock_data_sender():
    logger.info("MOCK DATA MODE: Starting sender.")
    while True:
        await asyncio.sleep(2)
        if not active_websocket_connections: continue
        mock_data = await generate_mock_output_data()
        mock_dict = mock_data.dict(exclude_none=True)
        mock_dict["is_mock_data"] = True
        for connection in active_websocket_connections:
            try: await connection.send_json(mock_dict)
            except: pass

app = FastAPI()
app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    # 1. ëª¨ì˜ ë°ì´í„° ìƒì„± íƒœìŠ¤í¬ (ì˜¤íƒ€ ìˆ˜ì •: asyncio.create_task)
    if MOCK_DATA_MODE:
        asyncio.create_task(mock_data_sender())
        logger.info("ğŸ“¡ ëª¨ì˜ ë°ì´í„° ì†¡ì‹  ì‹œì‘...")

    # 2. AI ëª¨ë¸ ë¡œë“œ
    if not load_ai_model_v2():
        logger.error("CRITICAL: AI ëª¨ë¸ V2 ë¡œë“œ ì‹¤íŒ¨!")

    # 3. Mobius ìë™ êµ¬ë… ì„¤ì • (ì‹¤ì‹œê°„ ì•„ë‘ì´ë…¸ ì—°ë™ìš©)
    # ë¦¬ë”ë‹˜, ngrok ì£¼ì†Œ ë°”ë€” ë•Œë§ˆë‹¤ ì—¬ê¸°ë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.
    CURRENT_NGROK_URL = "https://88d0c49bd9cc.ngrok-free.app/notification" 
    import random
    sub_name = f"sub_v3_{random.randint(1, 999)}"
    sub_url = f"https://onem2m.iotcoss.ac.kr/Mobius/{AE_NAME}/{CNT_NOISE}"
    sub_body = {
        "m2m:sub": {
            "rn": sub_name,
            "nu": [CURRENT_NGROK_URL],
            "nct": 1,
            "enc": {"net": [3]} # ContentInstance ìƒì„± ì‹œ ì•Œë¦¼ ì „ì†¡
        }
    }

    try:
        requests.delete(f"{sub_url}/sub_analysis_v3", headers=HEADERS, timeout=3)
        # ìƒˆ ì´ë¦„ìœ¼ë¡œ êµ¬ë… ì‹ ì²­
        response = requests.post(sub_url, headers=HEADERS, json=sub_body, timeout=3)
        
        if response.status_code == 201:
            logger.info(f"âœ… Mobius êµ¬ë… ì„±ê³µ! ì´ë¦„: {sub_name}")
        else:
            logger.error(f"âŒ êµ¬ë… ì‹¤íŒ¨(Status {response.status_code}): {response.text}")
    except:
        pass

# --- Helper Functions ---
def amplitude_to_db(amplitude: int) -> float:
    if amplitude == 0: return 0.0
    return 20 * np.log10(max(1, amplitude))

def analyze_vibration_peaks(vibration_z_list: np.ndarray, threshold: float = VIBRATION_PEAK_THRESHOLD) -> int:
    if vibration_z_list.size == 0: return 0
    peaks = np.where(vibration_z_list > threshold)[0]
    return len(peaks)

def create_waveform_image(audio_signature: List[float]) -> io.BytesIO:
    if not audio_signature: return None
    fig, ax = plt.subplots(figsize=(8, 2))
    ax.plot(audio_signature, color='#3182F6', linewidth=1)
    ax.set_title('Event Waveform', fontsize=10)
    ax.axis('off')
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
    plt.close(fig)
    buf.seek(0)
    return buf

# --- PDF Imports ---
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import inch, mm
from reportlab.lib import colors
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Image, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_CENTER
from reportlab.lib.utils import ImageReader
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

active_websocket_connections: List[WebSocket] = []

def create_noise_heatmap_image(logs: List[Dict]) -> io.BytesIO:
    if not logs: return None
    event_data = []
    for log in logs:
        severity = log.get("analysis", {}).get("severity")
        if severity in ["Red", "Yellow"]:
            try:
                ts = datetime.fromisoformat(log.get("timestamp").replace("Z",""))
                event_data.append({"weekday": ts.weekday(), "hour": ts.hour})
            except: continue
    if not event_data: return None
    df = pd.DataFrame(event_data)
    heatmap_data = df.pivot_table(index='weekday', columns='hour', aggfunc='size', fill_value=0)
    heatmap_data = heatmap_data.reindex(index=range(7), columns=range(24), fill_value=0)
    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
    fig, ax = plt.subplots(figsize=(8, 3))
    cax = ax.pcolormesh(heatmap_data.columns, heatmap_data.index, heatmap_data.values, cmap='YlOrRd', shading='auto')
    fig.colorbar(cax, label='Event Count')
    ax.set_title('Weekly Heatmap', fontsize=10)
    ax.set_yticks(np.arange(7) + 0.5)
    ax.set_yticklabels(days, fontsize=8)
    ax.invert_yaxis()
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150)
    plt.close(fig)
    buf.seek(0)
    return buf

def create_pie_chart_image(stats: Dict[str, int]) -> io.BytesIO:
    labels = list(stats.keys())
    sizes = list(stats.values())
    if sum(sizes) == 0: return None
    fig, ax = plt.subplots(figsize=(5, 3))
    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
    ax.axis('equal')
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=150)
    plt.close(fig)
    buf.seek(0)
    return buf

# --- Endpoints ---

@app.post("/notification/apology")
async def handle_apology_notification(request: Request):
    try:
        body = await request.json()
        logger.info(f"ğŸ“© [APOLOGY RECEIVED]: {body}")
        
        # 1. ì»¨í…ì¸  ì¶”ì¶œ (Mobius vs ì§ì ‘ ì „ì†¡ ëŒ€ì‘)
        sgn = body.get("m2m:sgn") or body.get("sgn")
        if sgn:
            rep = sgn.get("nev", {}).get("rep", {})
            content = rep.get("m2m:cin", {}).get("con") or rep.get("cin", {}).get("con")
        else:
            content = body

        # 2. JSON ë¬¸ìì—´ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        apology_dict = {}
        if isinstance(content, str):
            try: apology_dict = json.loads(content)
            except: apology_dict = {"message": content}
        else:
            apology_dict = content or {}

        # 3. ì‚¬ê³¼ ì‹ í˜¸ ì¡°ê±´ ê²€ì‚¬ (ê·œê²© í†µí•© ì²´í¬)
        is_ack = (
            apology_dict.get("apology_ack") == 1 or 
            apology_dict.get("event_type") == "apology_action" or 
            str(content).strip() in ["1", "Apology Sent"]
        )

        if is_ack:
            timestamp = apology_dict.get("timestamp") or datetime.now().isoformat()
            house_id = apology_dict.get("house_id", "Below_301") # ë¦¬ë”ë‹˜ ê°€êµ¬ IDì— ë§ê²Œ ìˆ˜ì •
            message = apology_dict.get("message", "ìœ„ì¸µì—ì„œ ì‚¬ê³¼ ë©”ì‹œì§€ë¥¼ ë³´ëƒˆìŠµë‹ˆë‹¤. ì†ŒìŒì´ ê³§ ì™„í™”ë  ì˜ˆì •ì…ë‹ˆë‹¤.")
            
            data = {
                "event": "apology", 
                "message": message, 
                "timestamp": timestamp,
                "house_id": house_id,
                "severity": "Green" # ì‚¬ê³¼ ìˆ˜ì‹  ì‹œ í™”ë©´ì„ ì§„ì •ì‹œí‚¤ëŠ” íš¨ê³¼ìš©
            }
            
            # íˆìŠ¤í† ë¦¬ì— ì €ì¥ (ë¦¬í¬íŠ¸ìš©)
            all_analysis_history.append(data)
            
            # WebSocket ì „íŒŒ
            if active_websocket_connections:
                await asyncio.gather(*[c.send_json(data) for c in active_websocket_connections], return_exceptions=True)
                logger.info(f"ğŸ“¢ [ì‚¬ê³¼ ì „íŒŒ ì™„ë£Œ] {house_id} -> ëŒ€ì‹œë³´ë“œ")

    except Exception as e:
        logger.error(f"âŒ Apology Error: {e}")
    
    return {"status": "ok"}

@app.post("/notification")
async def handle_mobius_notification(request: Request): 
    try:
        # [ìˆ˜ì •] ì—¬ê¸°ì„œë¶€í„° bodyë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        body = await request.json()
        if not body:
            return {"status": "ignored", "reason": "empty body"}

        
        sgn = body.get("m2m:sgn") or body.get("sgn")
        if not sgn: return {"status": "ignored"}
        
        rep = sgn["nev"]["rep"]
        raw_con = rep.get("m2m:cin", {}).get("con") or rep.get("cin", {}).get("con")
        
        # JSON ë¬¸ìì—´ì¸ ê²½ìš° ì•ˆì „í•˜ê²Œ íŒŒì‹±
        if isinstance(raw_con, str):
            try:
                payload_dict = json.loads(raw_con)
            except json.JSONDecodeError:
                logger.error("âŒ JSON íŒŒì‹± ì‹¤íŒ¨: ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return {"status": "error", "message": "Invalid JSON in con"}
        else:
            payload_dict = raw_con

        # New Payload Parse
        house_id = payload_dict.get("house_id", "unknown")
        timestamp = payload_dict.get("timestamp", datetime.now().isoformat())
        meta = payload_dict.get("meta", {})
        payload = payload_dict.get("payload", {})
        # 1. Data Prep & Validation (ë¦¬ë” ê·œë¦¬ë‹˜ ìˆ˜ì • ë²„ì „)
        # payload_dictì—ì„œ ì‹¤ì œ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” payload ë¶€ë¶„ì„ ë¨¼ì € ê°€ì ¸ì˜µë‹ˆë‹¤.
        sensor_data = payload_dict.get("payload", {}) 
        sound_raw = sensor_data.get("sound_raw", []) # payload ì•ˆì—ì„œ sound_raw ì¶”ì¶œ
        vibration_z = payload.get("vibration", {}).get("z", [])
        raw_max_amplitude = payload.get("raw_max_amplitude", 0)

        audio_np = np.array(sound_raw, dtype=np.float32)
        print(f"ğŸ” [DEBUG] ìˆ˜ì‹ ëœ ì˜¤ë””ì˜¤ ìƒ˜í”Œ ê°œìˆ˜: {len(audio_np)}ê°œ")
        # 3. ë°ì´í„° ë³´ì • (Zero-Padding)
        # [Validation] Data Length Check
        MIN_REQUIRED_SAMPLES = 10 # Lowered for testing connectivity
        if len(audio_np) < MIN_REQUIRED_SAMPLES:
            logger.warning(f"Skipping analysis: Data too short ({len(audio_np)})")
            return {"status": "skipped", "message": "Insufficient data length"}
        if len(audio_np) < 1000:
            audio_np = np.concatenate([audio_np, np.zeros(1000 - len(audio_np))])

        vibration_z = payload_dict.get("payload", {}).get("vibration", {}).get('z', [])
        vibration_np = np.array(vibration_z)
        # Calculate pure shock by removing 1.0g gravity component
        vibration_max = float(np.max(np.abs(vibration_np - 1.0))) if vibration_np.size > 0 else 0.0
        
        # Signature
        target_sig_len = 300
        audio_signature = audio_np.tolist()
        if len(audio_np) > target_sig_len:
            indices = np.linspace(0, len(audio_np)-1, target_sig_len).astype(int)
            audio_signature = audio_np[indices].tolist()

        # 2. AI Inference
        sr_val = meta.get("sampling_rate", "16000Hz")
        sr_int = int(str(sr_val).lower().replace("hz",""))
        audio_input = audio_np.flatten()
        processed = preprocess_audio_for_v2(audio_np, sr=sr_int)
        if hasattr(processed, 'numpy'): # í…ì„œ í˜•íƒœì¼ ê²½ìš°
            processed_input = processed.numpy().flatten()
        else: # ë„˜íŒŒì´ í˜•íƒœì¼ ê²½ìš°
            processed_input = np.array(processed).flatten()
        result_label, predicted_prob = predict_noise_v2(processed, sr=sr_int, vibration_z = vibration_z)
        
        logger.info(f"âœ… ë¶„ì„ ì„±ê³µ! ê²°ê³¼: {result_label} ({predicted_prob:.2f})")
    
        # 3. Grading (ë²•ì  ê¸°ì¤€ + ì§„ë™ í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§)
        raw_amp = payload.get("raw_max_amplitude", 0)
        calc_db = amplitude_to_db(raw_amp)
        num_peaks = analyze_vibration_peaks(vibration_np)
        
        # [ì‹ ê·œ] 1ë¶„/5ë¶„ í‰ê·  ê³„ì‚° (Leq_1min, Leq_5min)
        current_time = datetime.fromisoformat(timestamp.replace("Z",""))
        avg_1min, avg_5min = update_noise_metrics(house_id, calc_db)
        
        # ì‹œê°„ëŒ€ íŒŒì•… (ì£¼ê°„: 06~22ì‹œ, ì•¼ê°„: 22~06ì‹œ)
        current_hour = current_time.hour
        is_night = current_hour >= 22 or current_hour < 6
        
        # [Step 1] ë²•ì  ê¸°ì¤€ì¹˜ ì„¤ì •
        min_threshold = 34.0 if is_night else 39.0
        max_db_limit = 52.0 if is_night else 57.0 # Lmax ê¸°ì¤€
        suin_limit = 35.0 if is_night else 40.0 # ìˆ˜ì¸í•œë„
        airborne_limit = 40.0 if is_night else 45.0 # ê³µê¸°ì „ë‹¬ ì†ŒìŒ ê¸°ì¤€ (5ë¶„ í‰ê· )

        state = house_states.get(house_id, HouseState())

        # Lmax ì´ˆê³¼ íšŸìˆ˜ ì¹´ìš´íŠ¸
        if calc_db >= max_db_limit:
            state.lmax_exceed_count += 1
            logger.info(f"âš ï¸ [Lmax ì´ˆê³¼] {calc_db:.1f}dB ê°ì§€ (ëˆ„ì : {state.lmax_exceed_count}íšŒ)")

        # [ì‹ ê·œ] ë²•ì  ê²€í†  ë©”ì‹œì§€ ìƒì„±
        legal_review = []
        if avg_1min > suin_limit: legal_review.append("í™˜ê²½ë¶„ìŸì¡°ì •ìœ„ ìˆ˜ì¸í•œë„ ì´ˆê³¼")
        elif avg_1min > min_threshold: legal_review.append("ì§ì ‘ì¶©ê²© ì†ŒìŒ ì£¼ì˜ ë‹¨ê³„")
        
        if avg_5min > airborne_limit: legal_review.append("ê³µê¸°ì „ë‹¬ ì†ŒìŒ ê¸°ì¤€ ìœ„ë°˜")
        
        if calc_db > max_db_limit: legal_review.append(f"ìµœê³ ì†ŒìŒë„(Lmax) ê¸°ì¤€ ì´ˆê³¼ ê°ì§€")
        if state.lmax_exceed_count >= 3: legal_review.append("ìµœê³ ì†ŒìŒë„ ë°˜ë³µ ë°œìƒ (ë¶„ìŸ ì‹œ ë§¤ìš° ë¶ˆë¦¬)")

        review_msg = " | ".join(legal_review) if legal_review else "ë²•ì  ê¸°ì¤€ ì´ë‚´ (ì •ìƒ)"
        
        # [ì‹ ê·œ] ì™¸ë¶€ ì†ŒìŒ(thunderstorm, car_horn, siren) ì˜ˆì™¸ ì²˜ë¦¬
        external_noises = ["thunderstorm", "car_horn", "siren"]
        is_external = any(ext in result_label.lower() for ext in external_noises)

        if is_external:
            sev = "Green"
            logger.info(f"ğŸƒ [{house_id}] ì™¸ë¶€ ì†ŒìŒ ê°ì§€({result_label}): ë¬´ì¡°ê±´ Green íŒì •")
        elif calc_db < min_threshold and avg_1min < min_threshold:
            # ë²•ì  ê¸°ì¤€ ë¯¸ë‹¬ì´ë©´ AIê°€ ë­ë¼ê³  í•˜ë“  ë¬´ì¡°ê±´ Green (ê¸°ë¡ë§Œ í•¨, ì¤‘ì¬ ì•ˆ í•¨)
            sev = "Green"
            logger.info(f"[{house_id}] {calc_db:.1f}dB < {min_threshold}dB: ë²•ì  ê¸°ì¤€ì¹˜ ë¯¸ë‹¬ (Green)")
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ ê¸°ì¤€ì„ ë„˜ì—ˆìœ¼ë¯€ë¡œ Yellowë¡œ ì‹œì‘
            sev = "Yellow"
            
            # [Step 2] ì§„ë™ í•˜ì´ë¸Œë¦¬ë“œ ê²©ìƒ ë¡œì§ (vibration_max 0.2 ì´ìƒ ë˜ëŠ” AI ë°œë§ì¹˜ ê³ í™•ì‹  ì‹œ Red)
            is_foot = "footsteps" in result_label.lower()
            if vibration_max >= 0.2 or (is_foot and predicted_prob > 0.7) or avg_1min > suin_limit:
                sev = "Red"
                logger.info(f"ğŸš© [{house_id}] ê²©ìƒ: ì§„ë™, ë°œë§ì¹˜ ë˜ëŠ” ìˆ˜ì¸í•œë„ ì´ˆê³¼ë¡œ Red íŒì •")
            
            # [Step 3] ì§„ë™ì€ ì—†ì–´ë„ ì†ŒìŒ ìì²´ê°€ í•œê³„ì¹˜ë¥¼ ë„˜ì€ ê²½ìš° Red
            if calc_db >= max_db_limit:
                sev = "Red"
                logger.info(f"ğŸš© [{house_id}] ê²©ìƒ: ìµœê³ ì†ŒìŒë„({calc_db:.1f}dB) ì´ˆê³¼ë¡œ Red íŒì •")
        
        # 4. State Machine (ì§€ì† ì‹œê°„ ì²´í¬ ë° ìƒíƒœ ìœ ì§€)
        current_time = datetime.fromisoformat(timestamp.replace("Z",""))
        
        final_sev = sev
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (Loud/Quiet ìƒíƒœ ì „í™˜)
        if final_sev in ["Yellow", "Red"]:
            if state.status == "quiet":
                state.status = "loud"
                state.start_time = current_time
            state.last_loud_time = current_time
        else:
            if state.status == "loud" and (current_time - state.last_loud_time).total_seconds() >= 5:
                state.status = "quiet"
                state.start_time = None
        house_states[house_id] = state
        
        # 5. Post & Broadcast
        # [í•µì‹¬ ë¡œì§] í™•ì‹¤í•œ ì¤‘ì¬ ì œì–´: Yellow ë˜ëŠ” Redì¼ ë•Œë§Œ True, Greenì¼ ë•ŒëŠ” ë¬´ì¡°ê±´ False
        is_mediation_active = final_sev in ["Yellow", "Red"]

        # A. Status to CNT_STATUS (LED ì œì–´ìš© ë‹¨ìˆœ ë“±ê¸‰)
        status_data = {
            "event_id": f"STS_{datetime.now().strftime('%Y%m%d%H%M%S')}", 
            "house_id": house_id, 
            "grade": final_sev, 
            "db": calc_db, 
            "timestamp": timestamp
        }
        create_content_instance(status_data, labels=["grade"], container_name=CNT_STATUS)
        
        # B. Analysis ê²°ê³¼ êµ¬ì„± (ìƒì„¸ ë°ì´í„°)
        analysis_res = AnalysisResult(
            result=result_label, 
            probability=float(predicted_prob), 
            db_level=float(calc_db), 
            avg_1min=avg_1min,
            avg_5min=avg_5min,
            severity=final_sev,
            is_external=is_external,
            duration=0.0, 
            vibration_peaks=num_peaks, 
            vibration_max=vibration_max, 
            audio_signature=audio_signature
        )
        
        # C. ìµœì¢… ì¶œë ¥ ë°ì´í„° (ì¤‘ì¬ ìƒíƒœ í™•ì • ë° ë²•ì  ê²€í†  ë©”ì‹œì§€ í¬í•¨)
        output_data = OneM2MPlatformOutput(
            event_id=f"EVT_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            house_id=house_id, 
            timestamp=timestamp, 
            analysis=analysis_res,
            action=Action(
                mediation_sent=is_mediation_active, # ë“±ê¸‰ íŒì •ì— ë”°ë¥¸ ì •í™•í•œ ì¤‘ì¬ ì œì–´
                target=final_sev
            )
        )
        
        out_dict = output_data.dict(exclude_none=True)
        # ë²•ì  ê²€í†  ë©”ì‹œì§€ ì¶”ê°€
        out_dict["legal_review"] = review_msg
        out_dict["lmax_count"] = state.lmax_exceed_count
        
        all_analysis_history.append(out_dict)
        # ì„¸ì…˜ ë‚´ íˆìŠ¤í† ë¦¬ê°€ ì¶©ë¶„íˆ ìœ ì§€ë˜ë„ë¡ ì œí•œ ìƒí–¥ (ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ ë‚´)
        if len(all_analysis_history) > 5000:
            all_analysis_history.pop(0)

        
        # D. oneM2M ì €ì¥: ë¶„ì„ ê²°ê³¼ë§Œ ê¸°ë¡ (ì¤‘ì¬ ë°œì†¡ ìƒíƒœë¥¼ ë”°ë¡œ ë³´ë‚¼ í•„ìš” ì—†ìŒ)
        create_content_instance(out_dict, labels=["analysis"], container_name=CNT_NOISE)
        
        # E. ëŒ€ì‹œë³´ë“œ ì „íŒŒ: ì‹¤ì‹œê°„ìœ¼ë¡œ ì¤‘ì¬ ë°œì†¡ë¨ ìƒíƒœë¥¼ í™”ë©´ì— ë„ì›€
        for c in active_websocket_connections: 
            await c.send_json(out_dict)
        
        logger.info(f"ğŸš€ ì¤‘ì¬ ìƒíƒœ: {'ë°œì†¡' if is_mediation_active else 'ëŒ€ê¸°'} | ë“±ê¸‰: {final_sev}")
        return {"status": "success", "result": result_label, "mediation": is_mediation_active}

    except Exception as e: # ì—¬ê¸°ì„œ try ë¸”ë¡ì„ ì•ˆì „í•˜ê²Œ ë‹«ì•„ì¤ë‹ˆë‹¤.
        logger.error(f"Error: {e}")
        return {"status": "error"}
    
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    active_websocket_connections.append(websocket)
    try:
        while True: await websocket.receive_text()
    except: active_websocket_connections.remove(websocket)

@app.get("/get_latest_noise_data")
async def get_latest_noise_data():
    content = retrieve_latest_content_instance()
    if content: return content
    return {"analysis": {"result": "ëŒ€ê¸° ì¤‘", "db_level": 0, "severity": "Green"}}

@app.get("/logs")
async def get_logs(limit: int = 100): # ê¸°ë³¸ê°’ì„ 100ìœ¼ë¡œ ìƒí–¥í•˜ì—¬ ì´ˆê¸° ë¡œë“œì‹œ ë” ë§ì€ íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜´
    logs = retrieve_all_content_instances(limit=limit)
    if logs is not None:
        return {"status": "success", "logs": logs}
    # Only raise 500 if it's truly None (error), but retrieve_all... returns [] on valid empty.
    # So this might catch network errors which return [] too? 
    # mobius_client returns [] on error. Ideally we want to distinguish.
    # But for now, returning success with empty list is safer for the frontend.
    return {"status": "success", "logs": []}

def get_logs_for_report(house_id, start_dt, end_dt):
    local_logs = all_analysis_history
    platform_logs=retrieve_all_content_instances() or []
    combined_logs = local_logs + platform_logs
    if not combined_logs: return []
    
    filtered = []
    seen_events = set()
    for log in combined_logs:
        eid = log.get("event_id")
        if eid in seen_events: continue
        
        if log.get("house_id") != house_id: continue
        
        try:
            ts_str = log.get("timestamp").replace("Z", "")
            # UTC/KST ë³´ì •ì´ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ timedelta(hours=9)ë¥¼ ë”í•˜ì„¸ìš”
            ts = datetime.fromisoformat(ts_str) 
            if start_dt <= ts <= end_dt: 
                filtered.append(log)
                seen_events.add(eid)
        except: continue

    return sorted(filtered, key=lambda x: x.get("timestamp", ""), reverse=True)

@app.get("/report/csv")
def get_noise_degree(avg_1min, avg_5min, timestamp_str):
    """
    1ë¶„/5ë¶„ í‰ê·  ì†ŒìŒê³¼ ì‹œê°„ëŒ€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²•ì  ì†ŒìŒ ì •ë„ë¥¼ íŒì •í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # ì‹œê°„ëŒ€ íŒŒì•… (KST ê¸°ì¤€ ë³´ì • í•„ìš”ì‹œ í™•ì¸)
        ts = datetime.fromisoformat(timestamp_str.replace("Z", ""))
        is_night = ts.hour >= 22 or ts.hour < 6
    except:
        is_night = False

    # 1. ì§ì ‘ì¶©ê²© ì†ŒìŒ ê¸°ì¤€ (1ë¶„ í‰ê· )
    threshold_1min = 34 if is_night else 39
    limit_1min = 35 if is_night else 40  # ìˆ˜ì¸í•œë„ (ì°¸ì•„ì•¼ í•  í•œê³„)

    # 2. ê³µê¸°ì „ë‹¬ ì†ŒìŒ ê¸°ì¤€ (5ë¶„ í‰ê· )
    threshold_5min = 40 if is_night else 45

    status = []

    # íŒì • ë¡œì§
    if avg_1min > limit_1min:
        status.append(f"ìˆ˜ì¸í•œë„ ì´ˆê³¼(ê¸°ì¤€:{limit_1min}dB)")
    elif avg_1min > threshold_1min:
        status.append(f"ë²•ì  ì£¼ì˜(ê¸°ì¤€:{threshold_1min}dB)")

    if avg_5min > threshold_5min:
        status.append(f"ê³µê¸°ì „ë‹¬ ì†ŒìŒ ìœ„ë°˜(ê¸°ì¤€:{threshold_5min}dB)")

    if not status:
        return "ì •ìƒ(ìƒí™œì†ŒìŒ ë²”ìœ„)"
    
    return " | ".join(status)


async def get_csv_report(house_id: str, start_date: str, end_date: str):
    try:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.combine(datetime.strptime(end_date, "%Y-%m-%d"), dt_time.max)
    except: raise HTTPException(400, "Invalid date")
    logs = get_logs_for_report(house_id, start_dt, end_dt)
    output = io.StringIO()
    output.write(u'\ufeff')
    writer = csv.writer(output)
    writer.writerow(['timestamp', 'event', 'result', 'db', '1min_avg', '5min_avg', 'noise_degree', 'legal_review', 'lmax_count', 'prob', 'severity', 'vib_max', 'mediation'])
    for log in logs:
        a = log.get("analysis", {})
        ts = log.get("timestamp")
        degree = get_noise_degree(a.get("avg_1min", 0), a.get("avg_5min", 0), ts)
        writer.writerow([
            ts, log.get("event_id"), a.get("result"), a.get("db_level"),
            a.get("avg_1min", 0), a.get("avg_5min", 0), degree,
            log.get("legal_review", ""), log.get("lmax_count", 0),
            a.get("probability"), a.get("severity"), a.get("vibration_max", 0), log.get("action", {}).get("mediation_sent")
        ])
    output.seek(0)
    return StreamingResponse(iter([output.getvalue()]), media_type="text/csv", headers={"Content-Disposition": "attachment; filename=report.csv"})

try:
    pdfmetrics.registerFont(TTFont('Pretendard', 'Pretendard.ttf'))
    FONT_NAME = 'Pretendard'
except:
    # í°íŠ¸ íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ê°’
    FONT_NAME = 'Helvetica'
    logger.error("âŒ í•œê¸€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨! 'Pretendard.ttf' íŒŒì¼ í™•ì¸ í•„ìš”.")

@app.get("/report/pdf")
async def get_pdf_report(house_id: str, start_date: str, end_date: str):
    try:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.combine(datetime.strptime(end_date, "%Y-%m-%d"), dt_time.max)
    except: raise HTTPException(400, "Invalid date")

    logs = get_logs_for_report(house_id, start_dt, end_dt)
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    story = []
    styles = getSampleStyleSheet()

    korean_style = ParagraphStyle(
        name='KoreanStyle',
        fontName=FONT_NAME, # ë“±ë¡í•œ í°íŠ¸ ì ìš©
        fontSize=20,
        alignment=TA_CENTER,
        spaceAfter=20
    )
    
    # ì œëª©ì— ì ìš©
    story.append(Paragraph(f"ì¸µê°„ì†ŒìŒ ë¶„ì„ ë¦¬í¬íŠ¸: {house_id}", korean_style))
    
    # ë³¸ë¬¸ìš© ìŠ¤íƒ€ì¼ë„ í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€
    body_style = ParagraphStyle(
        name='BodyStyle',
        fontName=FONT_NAME,
        fontSize=10,
        leading=15
    )
    story.append(Paragraph(f"ì¸¡ì • ê¸°ê°„: {start_date} ~ {end_date}", body_style))

    
    # [ì¶”ê°€] ì¸µê°„ì†ŒìŒ ê¸°ì¤€ ì•ˆë‚´ í…ìŠ¤íŠ¸
    story.append(Spacer(1, 10))
    story.append(Paragraph("<b>[ì¸µê°„ì†ŒìŒ ë° ìˆ˜ì¸í•œë„ ê¸°ì¤€ ì•ˆë‚´]</b>", styles['Normal']))
    story.append(Paragraph("â€¢ ì£¼ê°„(06~22ì‹œ): 1ë¶„ í‰ê·  39dB ì´ˆê³¼ ì‹œ ë¬¸ì œ ì†ŒìŒ / ìˆ˜ì¸í•œë„ 40dB / 5ë¶„ í‰ê·  45dB ì´ˆê³¼ ì‹œ ì¸µê°„ì†ŒìŒ", styles['Normal']))
    story.append(Paragraph("â€¢ ì•¼ê°„(22~06ì‹œ): 1ë¶„ í‰ê·  34dB ì´ˆê³¼ ì‹œ ë¬¸ì œ ì†ŒìŒ / ìˆ˜ì¸í•œë„ 35dB / 5ë¶„ í‰ê·  40dB ì´ˆê³¼ ì‹œ ì¸µê°„ì†ŒìŒ", styles['Normal']))
    story.append(Paragraph("â€¢ ì†ŒìŒ ì˜ˆì‹œ: 30dB(ì¡°ìš©í•œ ì£¼íƒê°€), 40dB(ë‚®ì€ TV), 50dB(ë³´í†µ ëŒ€í™”), 60dB(ì‹ë‹¹ ëŒ€í™”)", styles['Normal']))
    story.append(Spacer(1, 10))

    # Add Heatmap
    hm = create_noise_heatmap_image(logs)
    if hm: story.append(Image(hm, width=6*inch, height=2.5*inch))
    
    # Add Waveform of Critical Event
    max_ev = None
    for log in logs:
        sev = log.get("analysis", {}).get("severity")
        if sev == "Red":
            max_ev = log
            break
    if max_ev:
        sig = max_ev.get("analysis", {}).get("audio_signature")
        wf = create_waveform_image(sig)
        if wf: story.append(Image(wf, width=6*inch, height=1.5*inch))
        
    # Add Table of Events
    if logs:
        story.append(Spacer(1, 12))
        story.append(Paragraph("Detailed Event Log", styles['Heading2']))
        table_data = [['Time', 'Type', 'Max', '1m Avg', '5m Avg', 'Degree', 'Sev']]
        for log in logs[:20]: # Show last 20 events in PDF for space
            a = log.get("analysis", {})
            ts_full = log.get("timestamp")
            ts = ts_full[11:19]
            degree = get_noise_degree(a.get("avg_1min", 0), a.get("avg_5min", 0), ts_full)
            legal = log.get("legal_review", "N/A")
            table_data.append([
                ts, a.get("result"), f"{a.get('db_level',0):.1f}", 
                f"{a.get('avg_1min',0):.1f}", f"{a.get('avg_5min',0):.1f}",
                degree, a.get("severity")
            ])
        t = Table(table_data, colWidths=[0.8*inch, 1.3*inch, 0.5*inch, 0.6*inch, 0.6*inch, 1.3*inch, 0.6*inch])
        t.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 8),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(t)
        
    doc.build(story)
    buffer.seek(0)
    return Response(content=buffer.getvalue(), media_type='application/pdf', headers={'Content-Disposition': 'attachment; filename=report.pdf'})

